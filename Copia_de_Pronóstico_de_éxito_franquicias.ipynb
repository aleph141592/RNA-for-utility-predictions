{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "UKggXjyBkMEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KrQEuNvsph2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a99f1e-5f4e-4054-d672-0ac99cd8d01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "from scipy.stats import f_oneway, pointbiserialr, chi2_contingency, ttest_ind\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargamos datos:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_U0V35vVs4je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = pd.read_csv('/content/drive/MyDrive/ARCHIVOS/score.csv', encoding='utf-8',index_col=False)\n",
        "scores\n"
      ],
      "metadata": {
        "id": "Qj0dwCDIM4jD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "7bd060e7-d2b5-4477-a99b-9ef1420c3941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ARCHIVOS/score.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c60135989006>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ARCHIVOS/score.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ARCHIVOS/score.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se asignaron etiquetas a las variables culitativas estado,municipio,colonia y tipo(desarrollo urbano). Para los estados y la colonia simplemente se enumeraron, pero para el caso de municipio se contaron los datos únicos y se ennumeró de esa forma. Por último, para el caso de tipo(desarrollo urbano) se asignaron de la siguiente forma:\n",
        "\n",
        "\n",
        "1.   Rural\n",
        "2.   En transición\n",
        "3.   Semi-urbano\n",
        "4.   Urbano\n",
        "5.   Semi-metrópoli\n",
        "6.   Metrópoli\n"
      ],
      "metadata": {
        "id": "C-GA26mOukbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Veamos algunas estadísticas de nuestro DataFrame\n",
        "scores.describe()\n"
      ],
      "metadata": {
        "id": "pnrq0hMcgXyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificamos que nuestro dataframe no contenga valores nulos para tener un óptimo desempeño\n",
        "scores.info()"
      ],
      "metadata": {
        "id": "AE05_bOUyjXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Matriz de correlación**"
      ],
      "metadata": {
        "id": "YT-rUl2zoqK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos la matriz de correlacion\n",
        "correlation_matrix = scores.corr(method='pearson')\n",
        "# Crear la gráfica con el tamaño deseado\n",
        "fig = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\",\n",
        "                labels=dict(color=\"Correlation\"),\n",
        "                width=1100, height=1100)  # Ajusta el tamaño según tus preferencias\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "BKhhwVycPfxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Midamos correlación de variables cualitativas (Estado, Municipio y Colonia)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PyQaOXAinSdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta primera parte nos dice la distribución conjunta de la variable objetivo Y con nuestra variable X6\n",
        "#Básicamente nos cuenta los valores obtenidos para Y por cada valor de nuestra variable X6\n",
        "CrosstabResult = pd.crosstab(index = scores['Y objetivo'], columns = scores['Estado'])\n",
        "print(CrosstabResult, '\\n')\n",
        "#El resultado de chi2_contingency contiene varios valores estadísticos, pero aquí estamos interesados en el p-valor.\n",
        "#Se obtiene mediante ChiSqResult[1]. Un p-valor pequeño (generalmente menor que un umbral predefinido, como 0.05) sugiere que hay una relación significativa entre las dos variables.\n",
        "#Un p-valor grande sugiere que no hay una relación significativa y, por lo tanto, la hipótesis nula no se rechaza.\n",
        "ChiSqResult = chi2_contingency(CrosstabResult)\n",
        "print('P-Value de Estado:', ChiSqResult[1])"
      ],
      "metadata": {
        "id": "1agUikoWnXvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CrosstabResult = pd.crosstab(index = scores['Y objetivo'], columns = scores['Municipio'])\n",
        "print(CrosstabResult, '\\n')\n",
        "#El resultado de chi2_contingency contiene varios valores estadísticos, pero aquí estamos interesados en el p-valor.\n",
        "#Se obtiene mediante ChiSqResult[1]. Un p-valor pequeño (generalmente menor que un umbral predefinido, como 0.05) sugiere que hay una relación significativa entre las dos variables.\n",
        "#Un p-valor grande sugiere que no hay una relación significativa y, por lo tanto, la hipótesis nula no se rechaza.\n",
        "ChiSqResult = chi2_contingency(CrosstabResult)\n",
        "print('P-Value de Municipio:', ChiSqResult[1])"
      ],
      "metadata": {
        "id": "gPzHJHhioM1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CrosstabResult = pd.crosstab(index = scores['Y objetivo'], columns = scores['Colonia'])\n",
        "print(CrosstabResult, '\\n')\n",
        "#El resultado de chi2_contingency contiene varios valores estadísticos, pero aquí estamos interesados en el p-valor.\n",
        "#Se obtiene mediante ChiSqResult[1]. Un p-valor pequeño (generalmente menor que un umbral predefinido, como 0.05) sugiere que hay una relación significativa entre las dos variables.\n",
        "#Un p-valor grande sugiere que no hay una relación significativa y, por lo tanto, la hipótesis nula no se rechaza.\n",
        "ChiSqResult = chi2_contingency(CrosstabResult)\n",
        "print('P-Value de Colonia:', ChiSqResult[1])"
      ],
      "metadata": {
        "id": "UwfaPpjCoNqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CrosstabResult = pd.crosstab(index = scores['Y objetivo'], columns = scores['Tipo'])\n",
        "print(CrosstabResult, '\\n')\n",
        "#El resultado de chi2_contingency contiene varios valores estadísticos, pero aquí estamos interesados en el p-valor.\n",
        "#Se obtiene mediante ChiSqResult[1]. Un p-valor pequeño (generalmente menor que un umbral predefinido, como 0.05) sugiere que hay una relación significativa entre las dos variables.\n",
        "#Un p-valor grande sugiere que no hay una relación significativa y, por lo tanto, la hipótesis nula no se rechaza.\n",
        "ChiSqResult = chi2_contingency(CrosstabResult)\n",
        "print('P-Value de Tipo de desarrollo urbano:', ChiSqResult[1])"
      ],
      "metadata": {
        "id": "V4hg_hOjoUTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Haremos un mapeo de la distribución de algunas de las variables para conocer cómo es dicha distribución"
      ],
      "metadata": {
        "id": "ijdyWdwx5nnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "\n",
        "ax1 = plt.subplot(2,4,1)\n",
        "ax2 = plt.subplot(2,4,2)\n",
        "ax3 = plt.subplot(2,4,3)\n",
        "ax4 = plt.subplot(2,4,4)\n",
        "ax5 = plt.subplot(2,4,5)\n",
        "\n",
        "ax1.hist(scores['Etiqueta tipo'], bins=30, color='green',edgecolor='purple', alpha=0.5)\n",
        "ax1.set_xlabel('Tipo de desarrollo urbano ', size=15)\n",
        "ax1.set_ylabel('Frequency', size=15)\n",
        "\n",
        "ax2.hist(scores['ENIGH'], bins=30, color='orange',edgecolor='purple', alpha=0.5)\n",
        "ax2.set_xlabel('ENIGH', size=15)\n",
        "\n",
        "ax3.hist(scores['Viviendas'], bins=30, color='blue',edgecolor='purple', alpha=0.5)\n",
        "ax3.set_xlabel('Número de viviendas', size=15);\n",
        "\n",
        "ax4.hist(scores['Poblacion'], bins=30, color='red',edgecolor='purple', alpha=0.5)\n",
        "ax4.set_xlabel('Población', size=15)\n",
        "\n",
        "ax5.hist(scores['Y objetivo'], bins=30, color='red',edgecolor='purple', alpha=0.5)\n",
        "ax5.set_xlabel('Probabilidad de éxito', size=15)"
      ],
      "metadata": {
        "id": "L5D6xYfa5xTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dividimos los datos para entrenamiento y validación\n",
        "Vemos que tenemos **105,952 registros y 36 columnas**, siendo una de las columnas nuestro valor objetivo (\"probabilidad objetivo\"). Lógicamente, utilizar la proporción convencional 80%-entrenamiento y  20%-validacion es un tanto excesivo así que **optaremos por utilizar 5,000 registros para validar y el resto será para entrenar a nuestra red neuronal.**"
      ],
      "metadata": {
        "id": "AssyYTbFbxNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecciona solo las columnas que deseas dividir en conjuntos de entrenamiento y validación\n",
        "data_to_split = scores.iloc[:, 4:]  # Esto excluye las primeras 4 columnas\n",
        "\n",
        "# Revuelve aleatoriamente los datos\n",
        "data_to_split = data_to_split.sample(frac=1, random_state=42)\n",
        "\n",
        "# Divide los datos en conjunto de entrenamiento y validación\n",
        "train_data, test_data = train_test_split(data_to_split, test_size=5000, random_state=42)\n",
        "\n",
        "print(\"Tamaño datos de entrenamiento:\", train_data.shape, \"Tamaño datos de validación:\", test_data.shape)"
      ],
      "metadata": {
        "id": "sZesSa1SkmSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizamos nuestros datos de entrenamiento"
      ],
      "metadata": {
        "id": "EFt7FzMpjHTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$x_{i,norm} = \\dfrac{x_{i}-\\mu}{\\sigma}$$\n",
        "    \n",
        "$$y_{i,norm} = \\dfrac{y_{i}-\\mu}{\\sigma}$$"
      ],
      "metadata": {
        "id": "90CefDc7_qA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_norm = (train_data - train_data.mean()) / train_data.std()\n",
        "#Veamos una muestra de nuestro nuevo dataframe normalizado:\n",
        "train_norm.head()"
      ],
      "metadata": {
        "id": "v3Xr4MbPf0ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5 color='RED'>\n",
        "Nota importante: La normalización de las muestras de prueba se realiza con los valores de $\\mu$ y $\\sigma$ obtenidos con las muestras empleadas para el entrenamiento"
      ],
      "metadata": {
        "id": "eT14AEnRA1Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_norm = (test_data - train_data.mean()) / train_data.std()\n",
        "test_norm.head()"
      ],
      "metadata": {
        "id": "-w-x_kPgA7v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separamos entre datos de entrada y la variable objetivo en ambos conjuntos"
      ],
      "metadata": {
        "id": "ButeE2FZomIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_norm.values[:,:-1]\n",
        "y_train = train_norm.values[:,-1:]\n",
        "print(type(x_train), type(y_train))\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "Bw54wazGoln3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_validation = test_norm.values[:,:-1]\n",
        "y_validation = test_norm.values[:,-1:]\n",
        "print(type(x_validation), type(y_validation))\n",
        "print(x_validation.shape)\n",
        "print(y_validation.shape)"
      ],
      "metadata": {
        "id": "0LY6K5jco4G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementamos nuestra red neuronal"
      ],
      "metadata": {
        "id": "uL2qhiP3kIg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "# Define el modelo de la red neuronal\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Capa de entrada con 36 unidades (una por cada columna en tus datos)\n",
        "model.add(layers.Input(shape=(40,)))\n",
        "\n",
        "# Capa oculta con 64 unidades y activación relu\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "# Capa de Dropout con una tasa del 0.2 (20% de las neuronas se apagan aleatoriamente)\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Capa de salida con 1 unidad para regresión\n",
        "model.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "# Compila el modelo especificando la función de pérdida, el optimizador y las métricas\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
        "\n",
        "# Entrena el modelo con tus datos de entrenamiento y etiquetas\n",
        "history = model.fit(train_norm, y_train, epochs=30, batch_size=32, validation_data=(test_norm, y_validation))"
      ],
      "metadata": {
        "id": "pS7iNUhJkNNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalúa el modelo en datos de validación (opcional)\n",
        "validation_loss, validation_mae = model.evaluate(x_validation, y_validation)\n",
        "print(f'Loss en datos de validación: {validation_loss}')\n",
        "print(f'MAE en datos de validación: {validation_mae}')\n",
        "\n",
        "# Realiza predicciones en nuevos datos (opcional)\n",
        "predictions = model.predict(norm_validation_data)"
      ],
      "metadata": {
        "id": "GnTHZsKCuhkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}